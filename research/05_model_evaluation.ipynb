{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/amoghagadde/Desktop/Amogha/Projects/Data_Science'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/amoghagadde/Desktop/Amogha/Projects'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_path: Path\n",
    "    tokenizer_path: Path\n",
    "    metric_file_name: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textSummarizer.constants import *\n",
    "from textSummarizer.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "        \n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            model_path = config.model_path,\n",
    "            tokenizer_path = config.tokenizer_path,\n",
    "            metric_file_name = config.metric_file_name\n",
    "           \n",
    "        )\n",
    "\n",
    "        return model_evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk, load_metric\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    \n",
    "    def generate_batch_sized_chunks(self,list_of_elements, batch_size):\n",
    "        \"\"\"split the dataset into smaller batches that we can process simultaneously\n",
    "        Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
    "        for i in range(0, len(list_of_elements), batch_size):\n",
    "            yield list_of_elements[i : i + batch_size]\n",
    "\n",
    "    \n",
    "    def calculate_metric_on_test_ds(self,dataset, metric, model, tokenizer, \n",
    "                               batch_size=16, device = torch.device(\"cpu\"),\n",
    "                            #    device=\"cuda\" if torch.cuda.is_available() else \"cpu\", \n",
    "                               column_text=\"article\", \n",
    "                               column_summary=\"highlights\"):\n",
    "        article_batches = list(self.generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
    "        target_batches = list(self.generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
    "\n",
    "        for article_batch, target_batch in tqdm(\n",
    "            zip(article_batches, target_batches), total=len(article_batches)):\n",
    "            \n",
    "            inputs = tokenizer(article_batch, max_length=1024,  truncation=True, \n",
    "                            padding=\"max_length\", return_tensors=\"pt\")\n",
    "            \n",
    "            summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
    "                            attention_mask=inputs[\"attention_mask\"].to(device), \n",
    "                            length_penalty=0.8, num_beams=8, max_length=128)\n",
    "            ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n",
    "            \n",
    "            \n",
    "            # Finally, we decode the generated texts, \n",
    "            # replace the  token, and add the decoded texts with the references to the metric.\n",
    "            decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, \n",
    "                                    clean_up_tokenization_spaces=True) \n",
    "                for s in summaries]      \n",
    "            \n",
    "            decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
    "            \n",
    "            \n",
    "            metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "            \n",
    "        #  Finally compute and return the ROUGE scores.\n",
    "        score = metric.compute()\n",
    "        return score\n",
    "\n",
    "\n",
    "    def evaluate(self):\n",
    "        # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        device = torch.device(\"cpu\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_path)\n",
    "        model_t5 = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_path).to(device)\n",
    "       \n",
    "        #loading data \n",
    "        dataset_samsum_pt = load_from_disk(self.config.data_path)\n",
    "\n",
    "\n",
    "        rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "  \n",
    "        rouge_metric = load_metric('rouge', trust_remote_code=True)\n",
    "\n",
    "        score = self.calculate_metric_on_test_ds(\n",
    "        dataset_samsum_pt['test'][0:10], rouge_metric, model_t5, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary= 'summary'\n",
    "            )\n",
    "\n",
    "        rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n",
    "\n",
    "        df = pd.DataFrame(rouge_dict, index = ['t5'] )\n",
    "        df.to_csv(self.config.metric_file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Values (length_penalty=0.8):**\n",
    "\n",
    "A value of 1.0 (default) means no penalty is applied.\n",
    "Values greater than 1.0 encourage shorter sequences. The model is penalized for generating longer sequences, leading to more concise summaries or responses.\n",
    "Values less than 1.0 encourage longer sequences. The model is less penalized for longer outputs, which can be useful if the task requires more detailed or verbose text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Values (num_beams=8):**\n",
    "\n",
    "num_beams = 1: This is equivalent to greedy search, where only the most probable sequence is kept at each step.\n",
    "num_beams > 1: Multiple sequences are explored simultaneously, leading to potentially more accurate or higher-quality outputs.\n",
    "Trade-offs:\n",
    "Higher values for num_beams typically lead to more accurate and diverse outputs but also increase computational cost and memory usage.\n",
    "\n",
    "Trade-offs:\n",
    "Higher values for num_beams typically lead to more accurate and diverse outputs but also increase computational cost and memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Values (, max_length=128):**\n",
    "\n",
    "Any positive integer value can be used. The appropriate value typically depends on the nature of the task:\n",
    "For summarization, a lower max_length might be appropriate.\n",
    "For text generation tasks that require detailed descriptions, a higher max_length might be used.\n",
    "Example:\n",
    "If max_length = 128, the model will stop generating text once it reaches 128 tokens, regardless of whether the sequence is complete or not.\n",
    "This can help prevent the model from generating overly verbose or redundant content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROUGE** (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics used to evaluate the quality of machine-generated text, particularly in the context of text summarization and machine translation. ROUGE compares the overlap between the generated text (e.g., a summary or translation) and one or more reference texts, usually created by humans. \n",
    "\n",
    "Here are the key ROUGE metrics:\n",
    "\n",
    "### 1. **ROUGE-N**\n",
    "- **Definition**: Measures the overlap of n-grams between the generated text and the reference text.\n",
    "- **Common Variants**:\n",
    "  - **ROUGE-1**: Measures the overlap of unigrams (single words).\n",
    "  - **ROUGE-2**: Measures the overlap of bigrams (two consecutive words).\n",
    "- **Example**:\n",
    "  - Reference: \"The cat is on the mat.\"\n",
    "  - Generated: \"The cat sat on the mat.\"\n",
    "  - ROUGE-1 would count the overlapping words like \"The,\" \"cat,\" \"on,\" \"the,\" and \"mat.\"\n",
    "  - ROUGE-2 would count overlapping bigrams like \"The cat\" and \"on the.\"\n",
    "\n",
    "### 2. **ROUGE-L**\n",
    "- **Definition**: Measures the longest common subsequence (LCS) between the generated and reference texts.\n",
    "- **Purpose**: Unlike ROUGE-N, which looks at contiguous sequences of words, ROUGE-L focuses on the longest sequence of words that appear in the same order in both texts. This is useful for capturing sentence structure and word order.\n",
    "- **Example**:\n",
    "  - Reference: \"The cat is on the mat.\"\n",
    "  - Generated: \"The mat is where the cat sat.\"\n",
    "  - The LCS is \"The cat is the mat,\" giving credit to the sequence of words that appear in both sentences.\n",
    "\n",
    "### 3. **ROUGE-Lsum**\n",
    "- **Definition**: A variant of ROUGE-L that is often used specifically for summarization tasks. It compares the longest common subsequence in a sentence-by-sentence manner between the generated and reference summaries.\n",
    "\n",
    "### 4. **ROUGE-W**\n",
    "- **Definition**: A weighted version of ROUGE-L that gives more importance to longer sequences.\n",
    "\n",
    "### 5. **ROUGE-S (ROUGE-Skip-Bigram)**\n",
    "- **Definition**: Measures the overlap of skip-bigrams, which are pairs of words that appear in the same order but are not necessarily adjacent.\n",
    "\n",
    "### **How ROUGE is Used**:\n",
    "- **Text Summarization**: ROUGE is widely used to evaluate automatic summarization systems by comparing generated summaries against one or more human-written reference summaries.\n",
    "- **Machine Translation**: In some cases, ROUGE is also used to evaluate translations by comparing them to reference translations.\n",
    "- **Evaluation**: ROUGE scores are generally reported as precision, recall, and F1 scores:\n",
    "  - **Precision**: The proportion of n-grams in the generated summary that are also in the reference summary.\n",
    "  - **Recall**: The proportion of n-grams in the reference summary that are also in the generated summary.\n",
    "  - **F1 Score**: The harmonic mean of precision and recall.\n",
    "\n",
    "### **Why ROUGE?**\n",
    "- **Efficiency**: ROUGE provides a quick and automatic way to evaluate the quality of generated text, which is especially useful when human evaluation is not feasible.\n",
    "- **Standard Metric**: It is a standard and widely accepted metric in the fields of natural language processing and text summarization.\n",
    "\n",
    "### **Limitations**:\n",
    "- **Surface-Level**: ROUGE focuses on surface-level similarities (exact word matches) and does not account for semantic meaning or coherence.\n",
    "- **Sensitivity to Synonyms**: It may not give credit for using synonyms or paraphrasing, even if the meaning is preserved.\n",
    "- **Context Ignorance**: ROUGE does not consider the broader context or the logical flow of the generated text.\n",
    "\n",
    "Despite its limitations, ROUGE remains a popular and useful metric, particularly when used in combination with other evaluation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-10 23:57:06,235: INFO: common: yaml file: /Users/amoghagadde/Desktop/Amogha/Projects/Data_Science/Text-Summarizer/config/config.yaml loaded successfully]\n",
      "[2024-08-10 23:57:06,237: INFO: common: yaml file: /Users/amoghagadde/Desktop/Amogha/Projects/Data_Science/Text-Summarizer/params.yaml loaded successfully]\n",
      "[2024-08-10 23:57:06,239: INFO: common: created directory at: artifacts]\n",
      "[2024-08-10 23:57:06,239: INFO: common: created directory at: /Users/amoghagadde/Desktop/Amogha/Projects/Data_Science/Text-Summarizer/artifacts/model_evaluation]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:37<00:00,  7.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-10 23:57:44,996: INFO: rouge_scorer: Using default tokenizer.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_evaluation_config = config.get_model_evaluation_config()\n",
    "    model_evaluation_config = ModelEvaluation(config=model_evaluation_config)\n",
    "    model_evaluation_config.evaluate()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
